import os
import sys
import socket
import subprocess
import time
import tempfile
import json
import dateutil.parser
import urllib.request
from pathlib import Path

import certifi
from packaging.requirements import Requirement
from packaging.utils import (
    canonicalize_name,
)


class Proxy:
    """
    Wrapper for mitmproxy.org

    We start an instance of mitmproxy to intercept requests by pip
    to ensure that pip doesn't see files which were published after
    the pypiSnapshotDate given to our script.
    It's generic enough that it should work with python mirrors
    besides pypi.org as well, but URLs for actual distribution files
    should not be intercepted for performance reasons and we
    currently just ignore files.pythonhosted.org by default.
    """

    def __init__(self, executable, args, env):
        self.env = env
        self.port = self.find_free_port()

        self.proc = subprocess.Popen(
            [
                executable,
                "--listen-port",
                str(self.port),
                "--anticache",
                *args,
            ],
            stdout=sys.stderr,
            stderr=sys.stderr,
            env=env,
        )
        self.wait("http://pypi.org", 10)
        self.cafile = self.generate_ca_bundle(".ca-cert.pem")

    def find_free_port(self):
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind(("", 0))
        port = sock.getsockname()[1]
        sock.close()
        return port

    def wait(self, test_url, timeout):
        """
        Wait for a bit until a given url is reachable via the proxy,
        as the latter starts asynchronous.
        """
        timeout = time.time() + timeout
        req = urllib.request.Request(test_url)
        req.set_proxy(f"127.0.0.1:{self.port}", "http")

        while time.time() < timeout:
            try:
                res = urllib.request.urlopen(req, None, 5)
                if res.status < 400:
                    break
            except urllib.error.URLError:
                pass
            finally:
                time.sleep(1)

    def generate_ca_bundle(self, path):
        """
        Because we only proxy *some* calls, but ignore i.e.
        files.pythonhosted.org we need to combine upstream ca certificates
        and the one generated by mitm proxy.
        """
        home = Path(self.env["HOME"])
        path = home / path
        with open(home / ".mitmproxy/mitmproxy-ca-cert.pem", "r") as f:
            mitmproxy_cacert = f.read()
        with open(certifi.where(), "r") as f:
            certifi_cacert = f.read()
        with open(path, "w") as f:
            f.write(mitmproxy_cacert)
            f.write("\n")
            f.write(certifi_cacert)
        return path

    def kill(self):
        self.proc.kill()


def nix_run(*args, check=True):
    return subprocess.run(
        ["nix", "--experimental-features", "nix-command flakes", *args],
        check=check,
        text=True,
        stdout=subprocess.PIPE,
        stderr=sys.stderr if check else subprocess.PIPE,
    )


def nix_flake_metadata(path):
    proc = nix_run("flake", "metadata", "--json", path, check=False)
    if proc.returncode != 0:
        return None
    return json.loads(proc.stdout)


def nix_show_derivation(path):
    proc = nix_run("show-derivation", path, check=False)
    if proc.returncode != 0:
        return None
    # attrs are keyed by derivation path, which we don't know,
    # but there should be only one in our case.
    return list(json.loads(proc.stdout).values())[0]


def lock_info_from_fod(store_path, drv_json):
    drv_out = drv_json.get("outputs", {}).get("out", {})
    assert str(store_path) == drv_out.get("path")
    assert "r:sha256" == drv_out.get("hashAlgo")
    url = drv_json.get("env", {}).get("urls")  # TODO multiple? commas?
    sha256 = drv_out.get("hash")
    if not (url and sha256):
        print(
            f"fatal: requirement '{store_path}' does not seem to be a FOD.\n",
            f"No URL ({url}) or hash ({sha256}) found.",
            file=sys.stderr,
        )
        sys.exit(1)
    return url, sha256


def path_from_file_url(url):
    prefix = "file://"
    prefix_len = len(prefix)
    if url.startswith(prefix):
        return Path(url[prefix_len:]).absolute()


def lock_info_from_path(full_path):
    current_flake = nix_flake_metadata(os.getcwd())
    assert current_flake

    # See whether it's a flake, and if so write the relative path into the lock
    # file.
    requested_flake = nix_flake_metadata(full_path)
    if requested_flake:
        if current_flake.get("path") == requested_flake.get("path"):
            flake_path = path_from_file_url(
                requested_flake.get("original", {}).get("url")
            )
            relative_path = full_path.relative_to(flake_path)
            del requested_flake["locks"]
            return str(relative_path), None
        else:
            print("requested flake")
            del requested_flake["locks"]
            print(json.dumps(requested_flake, indent=2))
            print(
                f"fatal: requirement '{full_path}' seems to refer to a flake which isn't ours.\n",  # noqa: E501
                file=sys.stderr,
            )
            sys.exit(1)

    # get just the "top-level" store path /nix/store/$hash-name/
    store_path = Path("/").joinpath(*full_path.parts[:4])
    if not Path("/nix/store") == store_path.parent:
        print(
            f"fatal: requirement '{full_path}' refers to something outside",
            "/nix/store which isn't a flake.",
            file=sys.stderr,
        )
        sys.exit(1)

    # use nix to print the derivation of our out_path in json
    # Assume it's a FOD and get its url and sha256
    drv_json = nix_show_derivation(store_path)
    if drv_json:
        return lock_info_from_fod(store_path, drv_json)
    else:
        print(
            f"fatal: requirement '{full_path}' refers to something we",
            "can't understand",
            file=sys.stderr,
        )
        sys.exit(1)


def lock_entry_from_report_entry(install):
    """
    Convert an entry of report['install'] to an object we want to store
    in our lock file, but don't add dependencies yet.
    """
    name = canonicalize_name(install["metadata"]["name"])
    download_info = install["download_info"]

    url, sha256 = download_info["url"], None
    full_path = path_from_file_url(url)
    if full_path:
        url, sha256 = lock_info_from_path(full_path)

    if not sha256:
        hash = (
            download_info.get("archive_info", {})
            .get("hash", "")
            .split("=", 1)  # noqa: 501
        )
        sha256 = hash[1] if hash[0] == "sha256" else None
    return name, dict(
        url=url,
        version=install["metadata"]["version"],
        sha256=sha256,
        dependencies=set(),
    )


def evaluate_extras(req, extras, env):
    """
    Given a python requirement string, a dictionary representing a python
    platform environment as in report['environment'], and a set of extras,
    we want to check if this package is required on this platform with the
    requested extras.
    """
    if not extras:
        return req.marker.evaluate({**env, "extra": ""})
    else:
        return any({req.marker.evaluate({**env, "extra": e}) for e in extras})


def evaluate_requirements(env, reqs, packages, root_name, extras, seen):
    """
    Recursively walk the dependency tree and check if requirements
    are needed for our current platform with our requested set of extras.
    If so, add them to our lock files dependencies field and delete
    requirements to save space in the file.
    A circuit breaker is included to avoid infinite recursion in nix.
    """
    if root_name in seen:
        print(
            f"fatal: cycle detected: {root_name} ({' '.join(seen)})",
            file=sys.stderr,  # noqa: E501
        )
        sys.exit(1)
    # we copy "seen", because we want to track cycles per
    # tree-branch and the original would be visible for all branches.
    seen = seen.copy()
    seen.append(root_name)

    for req in reqs[root_name]:
        if (not req.marker) or evaluate_extras(req, extras, env):
            req_name = canonicalize_name(req.name)
            packages[root_name]["dependencies"].add(req_name)
            evaluate_requirements(
                env, reqs, packages, req_name, req.extras, seen
            )  # noqa: 501


def lock_file_from_report(report):
    """
    Pre-process pips report.json for easier consumation by nix.
    We extract name, version, url and hash of the source distribution or
    wheel. We also preprocess requirements and their environment markers to
    the effective, platform-specific dependencies of each package. This makes
    heavy use of `packaging` which is hard to impossible to re-implement
    correctly in nix.

    We output a dictionary mapping normalized package names to a dict
    of version, url, sha256 and a list of normalized names of the packages
    effective dependencies on this platform and with the extras requested.

    This function can be further improved by also locking dependencies for
    non-selected extras, provided by our toplevel packages aka "roots".
    """
    packages = dict()
    # environment to evaluate pythons requirement markers in, contains
    # things such as your operating system, platform and python interpreter.
    env = report["environment"]
    # trace packages directly requested from pip to know where to start
    # walking the dependency tree.
    roots = dict()
    # packages in the report are a list, so we cache their requirement
    # strings in a list for faster lookups while we walk the tree below.
    requirements = dict()

    # iterate over all packages pip installed to find roots
    # of the tree and gather basic information, such as urls
    for install in report["install"]:
        name, package = lock_entry_from_report_entry(install)
        packages[name] = package
        requirements[name] = map(
            Requirement, install["metadata"].get("requires_dist", [])
        )
        if install.get("requested", False):
            roots[name] = install.get("requested_extras", set())

    # recursively iterate over the dependency tree from top to bottom
    # to evaluate optional requirements (extras) correctly
    for root_name, extras in roots.items():
        evaluate_requirements(
            env, requirements, packages, root_name, extras, list()
        )  # noqa: 501

    # iterate over the packages a third and final time to ensure that
    # dependencies are a stable sorted list, no matter in which order the
    # tree has been walked through.
    packages = {
        name: {**pkg, "dependencies": sorted(list(pkg["dependencies"]))}
        for name, pkg in packages.items()
    }
    return packages


def get_max_date(snapshot_date):
    try:
        return int(snapshot_date)
    except ValueError:
        return dateutil.parser.parse(snapshot_date)


def prepare_venv(venv_path, pip_version):
    subprocess.run([sys.executable, "-m", "venv", venv_path], check=True)
    subprocess.run(
        [
            f"{venv_path}/bin/pip",
            "install",
            "--upgrade",
            f"pip=={pip_version}",
        ],
        check=True,
        stdout=sys.stderr,
        stderr=sys.stderr,
    )
    return venv_path


if __name__ == "__main__":
    with open(sys.argv[1], "r") as f:
        args = json.load(f)

    with tempfile.TemporaryDirectory() as home:
        home = Path(home)

        print(
            f"selected maximum release date for python packages: {get_max_date(args['pypiSnapshotDate'])}",  # noqa: E501
            file=sys.stderr,
        )

        proxy = Proxy(
            executable=args["mitmProxy"],
            args=[
                "--ignore-hosts",
                ".*files.pythonhosted.org.*",
                "--script",
                args["filterPypiResponsesScript"],
            ],
            env={"pypiSnapshotDate": args["pypiSnapshotDate"], "HOME": home},
        )

        venv_path = prepare_venv(
            (home / ".venv").absolute(), args["pipVersion"]
        )  # noqa: 501

        flags = args["pipFlags"] + [
            "--proxy",
            f"https://localhost:{proxy.port}",
            "--progress-bar",
            "off",
            "--cert",
            proxy.cafile,
            "--report",
            str(home / "report.json"),
        ]
        for req in args["requirementsList"]:
            if req:
                flags.append(req)
        for req in args["requirementsFiles"]:
            if req:
                flags += ["-r", req]

        subprocess.run(
            [
                f"{venv_path}/bin/pip",
                "install",
                "--dry-run",
                "--ignore-installed",
                # "--use-feature=fast-deps",
                *flags,
            ],
            check=True,
            stdout=sys.stderr,
            stderr=sys.stderr,
        )
        proxy.kill()

        with open(home / "report.json", "r") as f:
            report = json.load(f)
        with open(os.getenv("out"), "w") as f:
            lock = lock_file_from_report(report)
            json.dump(lock, f, indent=2, sort_keys=True)
